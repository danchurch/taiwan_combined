{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One big biome table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am hoping to make comparisons of wood and leaf endophyte environmental patterns, I need to combine these datasets early in the biomformatics pipeline, to make them as comparable as possible. We'll try to stick to the [usearch (uparse)](http://drive5.com/usearch/) pipeline for the process, as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='environment'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working directory, on my machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cd /home/daniel/Documents/Taiwan_data/combined/combo_biome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the [usearch (uparse)](http://drive5.com/usearch/) pipeline, version usearch v8.1.1861_i86linux32 on my personal machine, and the equivalent 64-bit version on our computing cluster [ACISS](http://aciss-computing.uoregon.edu/). These programs are abbreviated (soft linked) to \"usearch81\" and \"usearch\", respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mergepairs'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging paired-end reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-pair all readsets in the same manner, except for Roo's stromatal readset, which he aligned by hand. \n",
    "\n",
    "First, the leaf study reads include a split 6+6 bp barcode scheme for identifying reads, so these need to be clipped from one read and combined on the other. I wrote a python script for this, let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "./BCunsplit4.py Roo_R2.fastq Roo_R1.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs two files, \"rearranged_Roo_R2.fastq\" and \"rearranged_Roo_R2.fastq\"\n",
    "\n",
    "Next we trim a little to make sure we doing our alignments with high quality base calls. The sites for trimming are decided by looking at the raw reads [(see below)](#quality), and finding where quality begins to drop off. \n",
    "To trim, we'll use the [FASTX-toolkit](http://hannonlab.cshl.edu/fastx_toolkit/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## wood\n",
    "fastx_trimmer -l 255 -i woodR1.fastq -o woodR1_trimmed.fastq\n",
    "fastx_trimmer -l 210 -i woodR2.fastq -o woodR2_trimmed.fastq\n",
    "\n",
    "## leaves. These lengths were decided by Roo\n",
    "fastx_trimmer -l 263 -i rearranged_Roo_R1.fastq -o Roo_R1_trimmed.fastq\n",
    "fastx_trimmer -l 170 -i rearranged_Roo_R2.fastq -o Roo_R2_trimmed.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual pairing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## wood, let's pair both trimmed and untrimmed just to compare:\n",
    "usearch -fastq_mergepairs woodR1.fastq -reverse woodR2.fastq  -fastqout woodtrimmedmerged.fastq -notrunclabels\n",
    "usearch -fastq_mergepairs woodR1.fastq -reverse woodR2.fastq -fastqout woodmerged.fastq -notrunclabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"-notrunclabels\" tag above asks usearch to keep the entire label of the forward reads, which is necessary because the wood reads, which are more recently sequenced than the leaf reads, contain sample info in their identifier lines. The leaves do not require this, their sample info is still in the sequence itself, to be use to [demultiplex](#demult) them, later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** typical report from these:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usearch v8.1.1803_i86linux64, 74.2Gb RAM, 12 cores\n",
    "(C) Copyright 2013-15 Robert C. Edgar, all rights reserved.\n",
    "http://drive5.com/usearch\n",
    "\n",
    "Licensed to: ronh@molbio.uoregon.edu\n",
    "\n",
    "<br>03:02 925Mb  100.0% 95.6% merged<br>5567799  Pairs (5.6M)<br>5323274  Merged (5.3M, 95.61%)<br>2067236  Alignments with zero diffs (37.13%)<br>0  Fwd tails Q <= 2 trimmed (0.00%)<br>15  Rev tails Q <= 2 trimmed (0.00%)<br>244525  No alignment found (4.39%)<br>0  Alignment too short (< 16) (0.00%)<br>4512849  Staggered pairs (81.05%) merged & trimmed<br>239.63  Mean alignment length\n",
    "259.16  Mean merged read length<br>3.81  Mean fwd expected errors<br>6.27  Mean rev expected errors<br>1.57  Mean merged expected errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers look pretty good. Many of the erroneous reads will be taken out below, in a [quality filtering](#qf) step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## leaves\n",
    "usearch -fastq_mergepairs Roo_R2.fastq -reverse Roo_R1.fastq -fastqout leafmerged.fastq\n",
    "usearch -fastq_mergepairs Roo_R2_trimmed.fastq -reverse Roo_R1_trimmed.fastq -fastqout leaftrimmedmerged.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did trimming do to our reads? Let's take a look. To plot these we'll use the [fastx wrapper](http://hannonlab.cshl.edu/fastx_toolkit/commandline.html#fastq_quality_boxplot_usage) for [gnuplot](http://www.gnuplot.info/), which I've tinkered with just a little to change up the crowded axes of the original setup. Fastx requires that we first compile the quality data from the fastq files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## wood quality stats:\n",
    "fastx_quality_stats -i woodR1.fastq -o woodR1_fastxstats.txt\n",
    "fastx_quality_stats -i woodR2.fastq -o woodR2_fastxstats.txt\n",
    "fastx_quality_stats -i woodR1_trimmed.fastq -o woodR1_fastxstats.txt\n",
    "fastx_quality_stats -i woodR2_trimmed.fastq -o woodR2_fastxstats.txt\n",
    "fastx_quality_stats -i woodmerged.fastq -o woodmerged_fastxstats.txt\n",
    "fastx_quality_stats -i woodtrimmedmerged.faestq -o woodtrimmedmerged_fastxstats.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## leaf quality stats:\n",
    "fastx_quality_stats -i Roo_R1.fastq -o Roo_R1_fastxstats.txt\n",
    "fastx_quality_stats -i Roo_R2.fastq -o Roo_R2_fastxstats.txt\n",
    "fastx_quality_stats -i Roo_R1_trimmed.fastq -o Roo_R1_trimmed_fastxstats.txt\n",
    "fastx_quality_stats -i Roo_R2_trimmed.fastq -o Roo_R2_trimmed_fastxstats.txt\n",
    "fastx_quality_stats -i leafmerged.fastq -o leafmerged_fastxstats.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the graphics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## wood graphics\n",
    "./dan_plot.sh -i woodR1_fastxstats.txt -o woodR1_quality.png\n",
    "./dan_plot.sh -i woodR2_fastxstats.txt -o woodR2_quality.png\n",
    "./dan_plot.sh -i  woodR1_trimmed_fastxstats.txt -o woodR1_trimmed_quality.png\n",
    "./dan_plot.sh -i  woodR2_trimmed_fastxstats.txt -o woodR2_trimmed_quality.png\n",
    "./dan_plot.sh -i woodmerged_fastxstats.txt -o woodmerged_quality.png\n",
    "./dan_plot.sh -i  woodtrimmedmerged_fastxstats.txt -o woodtrimmedmerged_quality.png\n",
    "\n",
    "## leaf graphics\n",
    "./dan_plot.sh -i  Roo_R1_trimmed_fastxstats.txt -o Roo_R1_trimmed_quality.png\n",
    "./dan_plot.sh -i  Roo_R2_trimmed_fastxstats.txt -o Roo_R2_trimmed_quality.png\n",
    "./dan_plot.sh -i leaftrimmedmerged_fastxstats.txt -o leaftrimmedmerged_quality.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='quality'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **untrimmed wood R1** reads look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='woodR1_quality.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to **trimmed wood R1** reads:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='woodR1_trimmed_quality.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **untrimmed wood R2** reads look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='woodR2_quality.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to **trimmed wood R2** reads:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='woodR2_trimmed_quality.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the **untrimmed, merged wood** file looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='woodmerged_quality.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously some problems here. So compare to the **merged, trimmed wood** reads:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='woodtrimmedmerged_quality.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much better, but still a large dip in quality around 15 bp. More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roo has already decided the trimming sites for his leaf data [(see above)](#mergepairs). Skip to the trimmed leaf R1 reads:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Roo_R1_trimmed_quality.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trimmed leaf R2 reads:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Roo_R2_trimmed_quality.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trimmed, merged leaf reads:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='leaftrimmedmerged_quality.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both leaves and wood, I think we've vastly improved the situation by merging. The nice thing about using the usearch merging algorithms is that they use a bayesian approach to calculating the q-scores of paired reads, so that agreements on base calls between R1 and R2 improve Q scores, and disagreements reduce them. The reduction by disagreement is proportional to the confidence of the two base calls at a site, so if the disagreement occurs at the end of a read, where quality is lower, the higher (more reliable) Q has a much greater influence on the final q-score of a base call. Check out the explanation [here](http://drive5.com/usearch/manual/exp_errs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qf'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the usearch pipeline, let's do some quality filtering. We'll use the [expected error approach](http://drive5.com/usearch/manual/exp_errs.html). We can set error cutoff of 1% of all bases in a read, meaning that a read of length 400 bp is thrown out if it likely contains 4 or more erroneous bases. I think this is permissable, given our OTU clustering will ultimately be done at 95% similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## wood sequences\n",
    "usearch -fastq_filter woodtrimmedmerged.fastq -fastq_maxee_rate .01 -fastqout wood_filtered.fastq -notrunclabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usearch v8.1.1803_i86linux64, 74.2Gb RAM, 12 cores\n",
    "(C) Copyright 2013-15 Robert C. Edgar, all rights reserved.\n",
    "http://drive5.com/usearch\n",
    "\n",
    "Licensed to: ronh@molbio.uoregon.edu\n",
    "\n",
    "01:04 857Mb  100.0% Filtering, 85.4% passed\n",
    "   5323274  FASTQ recs (5.3M)              \n",
    "   4548698  Converted (4.5M, 85.4%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## leaves\n",
    "usearch -fastq_filter leaftrimmedmerged.fastq -fastq_maxee_rate 0.01 -fastqout leaf_filtered.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usearch v8.1.1803_i86linux64, 529Gb RAM, 32 cores\n",
    "(C) Copyright 2013-15 Robert C. Edgar, all rights reserved.\n",
    "http://drive5.com/usearch\n",
    "\n",
    "Licensed to: ronh@molbio.uoregon.edu\n",
    "\n",
    "07:01 2.4Gb  100.0% Filtering, 90.7% passed\n",
    "  16701565  FASTQ recs (16.7M)             \n",
    "  15145323  Converted (15.1M, 90.7%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect these graphically as above, to see if avg read quality has improved. Not placing this here because the graphs look basically the same before and after filtering. But notice that we drop 15% of wood reads, and 9% of the leaf reads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='fastq2fasta'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert from fastq to fasta format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've merged paired ends and done some quality filtering to ensure that we've hopefully mostly eliminated sequencer error (hah! see [index bleed](#bleed) below), let's convert to fasta as required by most downstream steps. Using FASTX toolkit again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fastq_to_fasta -n -i leaf_filtered.fastq -o  leaf.fasta\n",
    "fastq_to_fasta -n -i wood_filtered.fastq -o  wood.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"-n\" flag tells fastx to retain sequences with \"N\" basecalls. Otherwise, these are removed, by default. Since ~1/2 of our leaf reads contain an \"N\", we need these. A single N basecall is an acceptable loss of information, OTU clustering and taxonomic assignments should be to deal with this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"demult\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Demultiplex leaf reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roo's leaf reads were prepped at an earlier date than the wood reads. At the time of their sequencing, the standard method for denoting sample identities was to look for the presence of 12 bp golay barcodes, the ones that we cut and pasted when we [merged paired end reads](#mergepairs). For probably the only time in this pipeline, we will use a [qiime](http://qiime.org/) script, [\"demultiplex_fasta.py\"](http://qiime.org/scripts/demultiplex_fasta.html) that was made to parse samples by these golay barcodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script requires a mapping file that lists the barcodes and their accompanying sample info, plus a \"linkerprimersequence\". I use a map file supplied by Roo. I do not know what a \"linkerprimersequence\" is, I believe this is supplied by the illumina software. The script seems to prefer .tsv format to .csv, and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#SampleID\tBarcodeSequence\tLinkerPrimerSequence\r\n",
      "1Leaf\tACCCATATATCC\tGCTGCGTTCTTCATCGATGC\r\n",
      "2Leaf\tACCCATAAGACG\tGCTGCGTTCTTCATCGATGC\r\n",
      "3Leaf\tTCGCCAGAACCA\tGCTGCGTTCTTCATCGATGC\r\n",
      "4Leaf\tACCCATATCAAA\tGCTGCGTTCTTCATCGATGC\r\n",
      "5Leaf\tACCCATATAGTA\tGCTGCGTTCTTCATCGATGC\r\n",
      "6Leaf\tACCCATCTACAG\tGCTGCGTTCTTCATCGATGC\r\n",
      "7Leaf\tACCCATCATACC\tGCTGCGTTCTTCATCGATGC\r\n",
      "8Leaf\tACCCATCATTAT\tGCTGCGTTCTTCATCGATGC\r\n",
      "9Leaf\tACCCATCTATCT\tGCTGCGTTCTTCATCGATGC\r\n"
     ]
    }
   ],
   "source": [
    "head leaf_sample_map.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "demultiplex_fasta.py -m leaf_sample_map.tsv -f leaf.fasta -o ./leaf_demult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a folder with a log file and a file called \"demultiplexed_seqs.fna\". I will rename this to \"leaf.fna\" and bring it into our working directory. We can remove primers from the sequences now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove primers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCR Primers in our wood reads were not included in these sequences, because they are used as part of the sequencing primers. The leaf reads, however, still contain our forward and reverse PCR primers. To remove these, we just clip the appropriate number of BPs from each end. We'll use the FASTX toolkit again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long are our primers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\r\n",
      "20\r\n"
     ]
    }
   ],
   "source": [
    "##ITS1F\n",
    "expr length \"CTTGGTCATTTAGAGGAAGTAA\"\n",
    "## ITS2\n",
    "expr length \"GCTGCGTTCTTCATCGATGC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fastx_trimmer -f 23 -i leaf.fna | fastx_trimmer -t 20 -o leaf_noprim.fna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating primers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both of our read sets, \"floating\" primer sequences appear. This happens in other studies, as indicated by [Bálint et al. (2014)](http://onlinelibrary.wiley.com/doi/10.1002/ece3.1107/abstract;jsessionid=FBCBBBE428CAA870889926051DBC9927.f04t02). As advised by these authors, I remove the sequences that contain these floating primers with a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## ITS1F\n",
    "grep CTTGGTCATTTAGAGGAAGTAA wood.fasta | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## ITS2\n",
    "grep GCTGCGTTCTTCATCGATGC wood.fasta | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not many. This doesn't include the reverse complements that also occur. To remove these floating primers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## wood\n",
    "./floatingprimers.py wood.fasta wood_defloat.fasta CTTGGTCATTTAGAGGAAGTAA GCTGCGTTCTTCATCGATGC\n",
    "## leaves\n",
    "./floatingprimers.py leaf.fna leaf_defloat.fna CTTGGTCATTTAGAGGAAGTAA GCTGCGTTCTTCATCGATGC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments for this script: floatingprimers.py input_fasta_file output_fasta_file forward_primer reverse_primer. \n",
    "\n",
    "Reverse compliments are checked automatically from the forward and reverse primer sequences that are given as arguments.\n",
    "\n",
    "When doing this, we lose 230 reads from the wood sequences (0.005%), and 312 from the leaves (0.002%). There were many more in the raw reads, but I think they were removed by the quality filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chimera checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to look for chimeras. We'll use the [uchime](http://www.drive5.com/usearch/manual/cmd_uchime_ref.html) algorithm, another step in the uparse/usearch pipeline. This is actually just the first of two checks for chimeras, the other being part of the [otu clustering](#otus).\n",
    "\n",
    "Get the latest [UNITE](https://unite.ut.ee/repository.php) database for usearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-08-21 18:27:34--  https://unite.ut.ee/sh_files/uchime_reference_dataset_01.01.2016.zip\r\n",
      "Resolving unite.ut.ee (unite.ut.ee)... 2001:bb8:2002:500:ec4:7aff:fe0a:37b2, 193.40.5.164\r\n",
      "Connecting to unite.ut.ee (unite.ut.ee)|2001:bb8:2002:500:ec4:7aff:fe0a:37b2|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 8233914 (7.9M) [application/zip]\r\n",
      "Saving to: ‘uchime_reference_dataset_01.01.2016.zip’\r\n",
      "\r\n",
      "\r",
      " 0% [                                       ] 0           --.-K/s              \r",
      " 1% [                                       ] 89,823       386KB/s             \r",
      " 2% [>                                      ] 220,895      384KB/s             \r",
      " 7% [=>                                     ] 597,727      766KB/s             \r",
      "14% [====>                                  ] 1,203,935   1.19MB/s             \r",
      "27% [=========>                             ] 2,268,895   1.86MB/s             \r",
      "43% [================>                      ] 3,612,383   2.53MB/s             \r",
      "61% [======================>                ] 5,054,175   3.08MB/s             \r",
      "78% [=============================>         ] 6,495,967   3.51MB/s             \r",
      "96% [====================================>  ] 7,954,143   3.86MB/s             \r",
      "100%[======================================>] 8,233,914   3.95MB/s   in 2.0s   \r\n",
      "\r\n",
      "2016-08-21 18:27:37 (3.95 MB/s) - ‘uchime_reference_dataset_01.01.2016.zip’ saved [8233914/8233914]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "wget https://unite.ut.ee/sh_files/uchime_reference_dataset_01.01.2016.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip this, then use the ITS1 only database to check for chimeras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## wood\n",
    "usearch -uchime_ref wood_defloat.fasta -db /home6/dthomas/combobiom/uchime_reference_dataset_01.01.2016/ITS1_ITS2_datasets/uchime_sh_refs_dynamic_develop_985_01.01.2016.ITS1.fasta -nonchimeras wood_notchim.fasta -strand plus -uchimeout woodchim_log.txt -notrunclabels\n",
    "\n",
    "## leaves\n",
    "usearch -uchime_ref leaf_defloat.fna -db /home6/dthomas/combobiom/uchime_reference_dataset_01.01.2016/ITS1_ITS2_datasets/uchime_sh_refs_dynamic_develop_985_01.01.2016.ITS1.fasta -nonchimeras leaf_notchim.fna -strand plus -uchimeout leafchim_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reads were chimeric? In the wood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "grep '>' wood_defloat.fasta | wc -l\n",
    "grep '>' wood_notchim.fasta | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4548468<br>\n",
    "4503552<br>\n",
    "Looks like we lost 44,916 reads (~1%) of the wood reads from the previous step.\n",
    "\n",
    "Leaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "grep '>' leaf_defloat.fna | wc -l\n",
    "grep '>' leaf_notchim.fna | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10820734<br>\n",
    "10684953<br>\n",
    "We lost 135,781 reads (~1%) of leaf reads from the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining study fasta files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the whole point of this exercise is get our three types of sequence files (Leaf endophytes, wood endophytes, and stromata) into a single fasta file that can be used to create OTU clusters. To help this, we'll simplify our leaf and wood identifier labels, and concatenate the three fasta files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying leaf endophytes identifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaf read identifiers look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "head -n 1 leaf_notchim.fna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\>78Leaf_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gnu [SED](https://www.gnu.org/software/sed/) to strip these down and reformat them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sed s/_.*//g leaf_notchim.fna > leaf_relab.fna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaf identifiers just contain sample number and study (host) info. They look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "head -n 1 leaf_relab.fna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\>78Leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying wood endophytes identifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wood identifiers look this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "head -n 1 wood_notchim.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\>M01498:244:000000000-ANT97:1:1101:17999:1109 1:N:0:160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a series of regexes that seems to work for simplifying them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sed s/^\\>.*[0-9]\\ //g wood_notchim.fasta | sed s/1:[YN]:0:/\\>/g | sed '/^>/ s/$/wood/g' > wood_relab.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "head -n 1 wood_relab.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\>160wood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining fasta files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine all of our fastas into a single fasta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cat wood_relab.fasta leaf_relab.fna Final_Stromata_Ref_Sequences.fasta > combo.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract ITS1 from sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ITS1 region from all reads will be extracted using [Bengtsson-Palme et al.'s (Nilsson's) ITS extractor](http://microbiology.se/software/itsx/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installation of the ITS extractor was theoretically simple but a little bit of a pain in the ass. It was simple in that you simply need to download a compressed, archived package from the [ITS extractor website](http://microbiology.se/software/itsx/), and it's main dependency, the [HMMER package](http://hmmer.org/). \n",
    "\n",
    "Each of these packages contains binaries that worked with my linux setups, but initial attempts at using the command line programs returned a lot of errors, so had do several things: delete old databases [(details here)](http://microbiology.se/2013/07/08/metaxa-and-hmmer-3-1b/) and use \"--reset T\" flag to create new dbs. I also had use the \"hmmpress\" command from the HMMER package with every one of these .hmm files, as a superuser, to create databases with correct formats. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "./ITSx -i combo.fasta -o combo_ITS --preserve T --allow_single_domain --t F -multi_thread 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes about the settings used here:\n",
    "\n",
    "--preserve T  == keep our identifier lines, don't let ITSx create new ones \n",
    "\n",
    "--allow_single_domain == if ITSx can only find one conserved region (18s, 5.8, or 28s) to anchor into, this is enough (usually require two)\n",
    "\n",
    "--t F == look at fungal reads only\n",
    "\n",
    "--multi_thread 12 == use 12 cores, cuz we got 'em."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
