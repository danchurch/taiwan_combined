{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One big biome table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am hoping to make comparisons of wood and leaf endophyte environmental patterns, I need to combine these datasets early in the biomformatics pipeline, to make them as comparable as possible. We'll try to stick to the [usearch (uparse)](http://drive5.com/usearch/) pipeline for the process, as much as possible.\n",
    "\n",
    "------------\n",
    "\n",
    "That was a year ago. \n",
    "\n",
    "Since then my car has been stolen, with my laptop in it. And the manuscripts which are based on these manuscripts are submitted. Somehow, the versions of this notebook and the following analysis notebook that were on github lost most of its graphic outputs, charts and maps and stuff. While most of the important graphics, etc, were backed up, I lost some of the computationally expensive intermediate files necessary to repopulate. \n",
    "\n",
    "So to make this notebook useable to reviewers and readers, I'll be picking through this process again, and the downstream analysis, if I don't go insane in the meantime. Maybe even if I do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Work environment](#environment)\n",
    "\n",
    "[Rearranging barcodes](#rearrange)\n",
    "\n",
    "[Trimming reads](#trim)\n",
    "\n",
    "[Merging paired-end reads](#mergepairs)\n",
    "\n",
    "[Quality filtering](#qf)\n",
    "\n",
    "[Convert form fastq to fasta format](#fastq2fasta)\n",
    "\n",
    "[Convert form fastq to fasta format](#fastq2fasta)\n",
    "\n",
    "[Demultiplex leaf reads](#demult)\n",
    "\n",
    "[Remove primers](#removeprimers)\n",
    "\n",
    "[Floating primers](#defloat)\n",
    "\n",
    "[Chimera checking](#chimera)\n",
    "\n",
    "[Combining fasta files from all 3 studies](#combine)\n",
    "\n",
    "[OTU clustering](#otuclust)\n",
    "\n",
    "[Customizing UNITE database](#UNITE)\n",
    "\n",
    "[Assign taxonomy](#asstax)\n",
    "\n",
    "[Make and tidy up biom table](#makebiom)\n",
    "\n",
    "[Add metadata](#addmetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='environment'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working directory, on my machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cd /home/daniel/Documents/Taiwan_data/combined/combo_biome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the [usearch (uparse)](http://drive5.com/usearch/) pipeline, version v8.0.1623_i86linux32, on the University of Oregon's Talapas computing cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rearrange'></a>\n",
    "### Rearranging barcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to merged paired end sequences of the leaves and wood. But before we can do this, there are several steps. First, the leaf study reads include a split 6+6 bp barcode scheme for identifying reads, so these need to be clipped from one read and combined on the other. I wrote a python script for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "\n",
      "## lets try to take a two unpaired read files, cut out the bp from the reverse,\n",
      "## and tack it onto the forward.\n",
      "## have to preserve the fastq format so that pandaseq can do unsplit3.py forward_reads reverse_reads\n",
      "\n",
      "#The first six bps and quality ratings of the reverse reads should be chopped off and placed after\n",
      "#the first six bps of the forward reads and quality ratings. For use with fastq files. It will spit\n",
      "#out two files, with the names: \"rearranged_[your orignial forward and reverse read file names].fastq\".\n",
      "\n",
      "import itertools ##to let us jump around \n",
      "from sys import argv\n",
      "\n",
      "script, forward_file, reverse_file = argv\n",
      "\n",
      "forwardlabels=[]\n",
      "reverselabels=[]\n",
      "forwardreads=[]\n",
      "forwardreadsq=[]\n",
      "reversereads=[]\n",
      "reversereadsq=[]\n",
      "forwardBC=[]\n",
      "forwardBCq=[]\n",
      "reverseBC=[]\n",
      "reverseBCq=[]\n",
      "\n",
      "with open(forward_file) as foop:\n",
      "\n",
      "\t#labels:\n",
      "\n",
      "\tfor h in itertools.islice(foop, 0, None, 4):\n",
      "\t\tforwardlabels.append(h)\n",
      "\n",
      "##forward sequencies, barcodes: \n",
      "\n",
      "\tfoop.seek(0)\n",
      "    \n",
      "\tfor i in itertools.islice(foop, 1, None, 4):\n",
      "\t\tforwardreads.append(i[6:])        \n",
      "\t\tforwardBC.append(i[0:6])\n",
      "\n",
      "##forward quality info:\n",
      "\n",
      "\tfoop.seek(0)\n",
      "\n",
      "\tfor j in itertools.islice(foop, 3, None, 4):\n",
      "\t\tforwardreadsq.append(j[6:])\n",
      "\t\tforwardBCq.append(j[0:6])\n",
      "\n",
      "## reverse reads and sequences:\n",
      "\n",
      "with open(reverse_file) as coop:\n",
      "\n",
      "#labels:\n",
      "\n",
      "\tfor h in itertools.islice(coop, 0, None, 4):\n",
      "\t\treverselabels.append(h)\n",
      "\n",
      "##Reverse reads and sequences: \n",
      "\n",
      "\tcoop.seek(0)\n",
      "    \n",
      "   \n",
      "\tfor i in itertools.islice(coop, 1, None, 4):\n",
      "\t\treversereads.append(i[6:])        \n",
      "\t\treverseBC.append(i[0:6])\n",
      "\n",
      "## reverse quality info\n",
      "\n",
      "\tcoop.seek(0)\n",
      "\n",
      "\tfor j in itertools.islice(coop, 3, None, 4): \n",
      "\t\treversereadsq.append(j[6:])\n",
      "\t\treverseBCq.append(j[0:6])\n",
      "\n",
      "##do I need the reverse compliment on these? Don't think so, guess I'll find out. \n",
      "##Now, to combine into a new fastq file\n",
      "\n",
      "ee=['rearranged_', forward_file]\n",
      "ff=''.join(ee)\n",
      "\n",
      "with open(ff,'w') as foop2:\n",
      "\n",
      "\tfor j in range(len(forwardreads)):\n",
      "\t\t    \n",
      "\t\tprint(forwardlabels[j], file = foop2, end = '')\n",
      "\t\tprint(forwardBC[j]+reverseBC[j]+forwardreads[j], sep = '', file = foop2, end='') \n",
      "\t\tprint('+', file = foop2)\n",
      "\t\tprint(forwardBCq[j]+reverseBCq[j]+forwardreadsq[j], file = foop2, end = '')\n",
      "\n",
      "gg=['rearranged_', reverse_file]\n",
      "hh=''.join(gg)\n",
      "\n",
      "with open(hh,'w') as goop2:\n",
      "\n",
      "\tfor j in range(len(reversereads)):\n",
      "\t    \n",
      "\t\tprint(reverselabels[j], file = goop2, end = '')\n",
      "        ## this line is edited to keep barcodes in reverse reads:\n",
      "\t\tprint(forwardBC[j]+reverseBC[j]+reversereads[j], file = goop2, end='')\n",
      "\t\tprint('+', file = goop2)\n",
      "\t\t#print(reversereadsq[j], file = goop2, end = '')\n",
      "        ## this line above replaced with line below, also \n",
      "        ## to keep barcodes in reverse reads:\n",
      "\t\tprint(forwardBCq[j]+reverseBCq[j]+reversereadsq[j], file = goop2, end = '')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##a function for reverse complimenting (may need this?):\n",
      "\n",
      "#def revcompl(x): \n",
      "\n",
      "#    aa=''.join([{'A':'T','C':'G','G':'C','T':'A'}[B] for B in x][::-1])\n",
      "#    return(aa)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "cat scripts/BCunsplit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have details on how I used this [here](https://github.com/danchurch/taiwan_dada2/blob/master/dada2pipeline.ipynb). \n",
    "\n",
    "This outputs two files, \"rearranged_Roo_R2.fastq\" and \"rearranged_Roo_R2.fastq\". I did this in another directory, so we'll add some sym links here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## leaves\n",
    "ln -s /home/daniel/Documents/taiwan/taiwan_dada2/rearranged_leafR1.fastq reLeafR1.fastq\n",
    "ln -s /home/daniel/Documents/taiwan/taiwan_dada2/rearranged_leafR2.fastq reLeafR2.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trim'></a>\n",
    "### Trimming reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we trim a little to make sure we doing our alignments with high quality base calls. The sites for trimming are decided by looking at the raw reads [(see below)](#quality), and finding where quality begins to drop off. \n",
    "To trim, we'll use the [FASTX-toolkit](http://hannonlab.cshl.edu/fastx_toolkit/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our wood reads are already demultiplexed, so we don't have a single forward and reverse read file for all of our wood samples, like we do above with the leaves. So let's make a script for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trims.sh\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## wood reads live here:\n",
    "wooddir=/home/daniel/Documents/taiwan/woodreads/\n",
    "\n",
    "## working directory is here:\n",
    "cd /home/daniel/Documents/taiwan/taiwan_combined_biom\n",
    "\n",
    "###### R1 reads:\n",
    "\n",
    "## home for trimmed R1 wood reads here:\n",
    "R1trimdir='/home/daniel/Documents/taiwan/taiwan_combined_biom/trimmed_wood/R1/'\n",
    "\n",
    "## trim just the R1s, output to their new home with new filename:\n",
    "for i in $wooddir*_R1_*; do\n",
    "    echo $i\n",
    "    out=$R1trimdir$(basename ${i/_001\\.fastq/_trimmed\\.fastq})\n",
    "    fastx_trimmer -l 255 -i $i -o $out && echo $out \n",
    "done\n",
    "\n",
    "###### R2 reads:\n",
    "\n",
    "## home for trimmed R2 wood reads here:\n",
    "R2trimdir='/home/daniel/Documents/taiwan/taiwan_combined_biom/trimmed_wood/R2/'\n",
    "\n",
    "## trim just the R2s, output to their new home with new filename:\n",
    "for j in $wooddir*_R2_*; do\n",
    "    echo $j\n",
    "    out=$R2trimdir$(basename ${j/_001\\.fastq/_trimmed\\.fastq})\n",
    "    fastx_trimmer -l 210 -i $j -o $out && echo $out \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## leaves. These lengths were decided by Roo. They are all still in one pile:\n",
    "fastx_trimmer -l 263 -i reLeafR1.fastq -o Roo_R1_trimmed.fastq\n",
    "fastx_trimmer -l 170 -i reLeafR2.fastq -o Roo_R2_trimmed.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mergepairs'></a> \n",
    "\n",
    "### Merging paired-end reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/env bash\n",
    "\n",
    "#SBATCH --job-name=merge_wood\n",
    "#SBATCH --output=merge_wood.out\n",
    "#SBATCH --error=merge_wood.err\n",
    "#SBATCH --time=0-05:00:00\n",
    "#SBATCH --nodes=1\n",
    "\n",
    "module load usearch/8.0\n",
    "\n",
    "cd projects/xylaria/dthomas/\n",
    "\n",
    "R1d=/projects/xylaria/dthomas/trimmed_wood/R1/\n",
    "\n",
    "for forward in $R1d*; do\n",
    "    echo $forward\n",
    "    reverse=${forward//R1/R2}\n",
    "    aa=$(basename $forward); output=\"/projects/xylaria/dthomas/merged_wood/\"${aa/_R1_trimmed.fastq/_merged.fastq}\n",
    "    usearch -fastq_mergepairs $forward -reverse $reverse -fastqout $output\n",
    "done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
